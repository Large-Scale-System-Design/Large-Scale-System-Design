## 8주차. 키-값 저장소 설계

<br>

1. 키 값 저장소
    - 비 관계형 DB
    - 고유 식별자를 키로 가져야 함
    - 성능상의 이유로 키는 짧을수록 좋음
    - dynamo, redis, memcache 등
2. 키 값 저장소 설계
    - 데이터 압축
    - 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장
    - 분산 키-값 저장소
3. CAP
    - 일관성(Consistency)
        - 분산 환경에서 어느곳에 접근하든 언제나 같은 데이터를 보게 되어야 함
    - 가용성(Availability)
        - 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야 함
    - 파티션 감내(Partition Tolerance)
        - 파티션이 생기더라도(두 노드 사이에 통신 장애가 생기더라도) 시스템은 계속 동작하여야 함

4. 일관성 or 가용성 뭘 포기할건데?
    - 일관성: 3대 노드 중 1대가 장애가 나면, 이 문제가 해결될때까지 나머지 노드들도 오류를 반환해야함.
        - 은행권
    - 가용성: 최신 데이터를 들고 있는 노드에 장애가 나면, 나머지 노드가 오래된 데이터라도 응답
        - AP 시스템

5. 키-값 저장소 시스템 컴포넌트
    - 데이터 파티션
        - 데이터를 작은 파티션들로 분할한 여러 서버에 저장
            - 안정 해시 설계 내용
                - 데이터를 여러 서버에 고르게 분산할 수 있는가
                - 노드가 추가/삭제될 떄 데이터의 이동을 최소화할 수 있는가
            - 규모 확장 자동화
                - 시스템 부하에 따라 서버가 자동으로 추가/삭제
                - 성능에 따라 가상 노드를 추가해서 더 많은 데이터 저장(더 많은 가상 노드)

    - 데이터 다중화(복제)
        - 안정 해시 설계에서 찾은 노드로부터 N개 정도의 서버에 데이터 사본을 보관
        - 가상 노드를 사용하는 경우, 동일한 노드가 중복 선택될 수 있기에 이를 방지해야 함
        - 다른 데이터 센터에 분산 저장 및 센터간 고속 네트워크 연결
    - 일관성
        - 여러 노드에 다중화(복제)된 데이터는 적절히 동기화가 되어야 함
        - 정족수 접근법: N < W + R
            - N: 사본 개수
            - W: 쓰기 연산에 대한 정족수(최소 W개의 노드에서 쓰기 연산이 성공했다는 응답)
            - R: 읽기 연산에 대한 정족수(최소 R개의 노드에서 읽기 연산이 성공했다는 응답)
        - 응답 지연과 데이터 일관성 사이의 타협점
            - 일관성을 보장한다라고 모든 노드의 응답을 다 기다리려면 무한정 기다려야 되고 라면물 올려야 함.
        - W가 작으면 빠른 쓰기 연산이 필요한거고, R이 작으면 빠른 읽기 연산이 필요한거고
        - 일관성 모델
            - 강함: 모든 읽기 연산이 가장 최근에 갱신된 결과를 응답
            - 약함: 강함의 반대 
                - 대중적인 Dynamo, Casandra 는 결과적 일관성이라는 약한 일관성이지만, 결과적으로 모든 사본에 동기화 되는 모델

    - 일관성 불일치 해소
        - 데이터 버저닝
            - 데이터가 변경될 때마다 새로운 버전을 만드는 것
        - 벡터 시계
            - 어떤 버전이 더 최신인지 확인하는 방법
                - 클라이언트에서 이 충돌을 감지하고 백터 시계를 사용해 충돌을 해소해야 하므로 클라이언트 구현이 복잡
                - 충돌나는 순서쌍이 빨리 늘어나기 때문에 임계치 설정 필요
                    - 임계치 이상 길어지면 오래된 데이터를 지우는데 → 이렇게 하면 정확하지 않을 수 있지만 실제로 그런 사례가 없다고 함(Dynamo 왈)

    - 장애 처리 → 장애 처리
        - 장애 감지
            - 보통 2대 이상의 서버가 특정 서버가 장애났다고 응답해야 실제로 장애가 발생했다고 간주 
            - 모든 노드 사이를 연결하는 것이 장애를 감지하는 손쉬운 방법 → 서버가 많으면 비효율적임
                - 가십 프로토콜(분산형 장애 감지)를 도입
                    - 각 노드는 멤버십 목록을 유지(멤버ID > heartbeat)
                    - 각 노드는 주기적으로 자신의 heartbeat 를 증가
                    - 각 노드는 무작위로 선정된 노드들에게 주기적으로 자신의 heartbeat 목록을 보냄
                    - 이걸 받은 노드들은 멤버십 목록을 최신으로 갱신
                    - 지정된 시간동안 갱신되지 않은 멤버는 장애 상태인걸로 간주함

        - 장애 처리
            - 임시 장애 처리: hinted handoff
                - 정족수 접근법을 사용해 건강한(?) 살아있는(?) W개의 서버와 R개의 서버들이 장애서버의 똥을 치움
                    - 그리고 장애서버가 복구되면 데이터를 일괄 반영해서 복구시킴

            - 영구 장애 처리: 
                - 사본들을 비교하여 최신 버전으로 갱신
                - 사본 간의 일관성이 망가진 상태를 탐지하고 전송 데이터의 양을 줄이기 위해 머클 트리(해시 트리)를 사용
                - 머클 트리(Merkle)

<br>

머클 트리

'merkle'는 주로 블록체인 분야에서 \*\*머클트리(Merkle Tree)\*\*의 이름으로 사용되며, 이는 블록 내 모든 거래 내역을 하나의 해시 값으로 요약하는 자료 구조를 의미합니다. 이 트리 구조는 데이터를 압축하고 효율적으로 관리하며, 개별 데이터의 위변조 여부를 빠르게 확인할 수 있게 해줍니다. 머클트리의 발명가인 \*\*랄프 머클(Ralph Merkle)\*\*의 이름을 딴 용어입니다.  

<br>

1\. 어떤 특정 도메인(거래 내역 등)에 대해서 모두 원본 그대로인지 하나하나 비교해서 확인하려면 매우 느림

개별 데이터 조각: Hash(이름) / Hash(나이) / Hash(성별) / Hash(전화번호)

→ Hash 해버려: H1 / H2 / H3 / H4

→ Hash(H1 + H2) + Hash(H3 + H4)

→ Hash(Hash(H1+H2) + Hash(H3+H4)) 이런식으로 트리구조로 올라가면서 최상위 해시값을 남김(Merkle Root)

<br>

수많은 필드 중 1개만 바뀌더라도 (성별이 남 → 여로 바뀌더라도) 최종 해시값만 비교해벌이면 → 변조되었는지 바로 확인이 가능하다 이말임

<br>

이걸로 장애 처리를 어떻게 사용하는지?

-> 두 서버간에 데이터를 Merkle Root 로 데이터가 같은지 확인하고 / 해시 데이터가 다른부분만 하위 hash(지문이라고도 표현)로 내려가면서 어떤 데이터가 다른건지 확인해서 변조를 확인하는 것

<br>

> ### 요약
> 
> <br>
> 
> 키 값 저장소(K-V Store)에서 영구 장애가 발생하거나 데이터 불일치가 생겼을 때,
> 
> 1. 서버들은 각자 가진 데이터로 **머클 트리를 만듭니다.**
> 2. 두 서버는 전체 데이터를 비교하는 대신, 트리의 \*\*최상위 '루트 해시(지문)'\*\*만 비교합니다.
> 3. \*\*(같으면) 1초 만에 "데이터 동일함"\*\*을 확인하고 끝냅니다.
> 4. **(다르면) 지문이 다른 '가지'만** 계속 파고 내려가서, **정확히 어떤 '잎(Key-Value)'이 다른지** 최소한의 비교로 찾아냅니다.
> 5. 그렇게 찾아낸 **다른 데이터 조각만** 전송받아 복구(동기화)합니다.
> 
> 이것이 머K-V 저장소의 **안티-엔트로피(Anti-Entropy) 또는 '리페어(Repair)'** 메커니즘이며, 아마존 다이나모(Dynamo)나 아파치 카산드라(Cassandra) 같은 시스템에서 핵심적으로 사용되는 기술입니다.

<br>
- 데이터 센터 장애 처리(데이터 센터 분산)
<br>
- 시스템 아키텍처 다이어그램(총정리)
<br>
    - 정족수(중재자) 활용 + 안정 해시 + 노드에 대한 분산 + 데이터에 대한 다중화(복제) + SPOF 금지(장애 처리) + 
    <br>
        - 쓰기 경로: 메모링 적재 / 메모리 부족하면 String Sorted Table(별도 공간)에 따로 저장
        <br>
        - 읽기 경로: 메모리에 있는지 확인  →  없으면 SSTable에서 확인(블룸 필터)
        <br>
<br>
