# 주제
- 9장: 웹 크롤러 설계

# 개요
- 웹 크롤러: 새로운 / 갱신된 웹 사이트 컨텐츠를 찾아내는 프로그램
	- 웹 페이지 / 이미지 / 비디오 / PDF 등
## 종류
- 검색 엔진 인덱싱: 검색 엔진 로컬 인덱스 생성 목적
- 웹 아카이빙: 웹 사이트 정보 장기보관 목적
- 웹 마이닝: 데이터 수집을 통한 유용한 지식 도출 목적
- 웹 모니터링: 상표권 / 저작권 침해 사례 모니터링
# 1. 문제 이해 및 설계 범위 확정
## 기본 알고리즘
1. 입력된 URL 집합이 가리키는 모든 웹 페이지 다운로드
2. 다운받은 웹 페이지 내 URL 추출
3. 추출된 URL을 다운로드 대상 URL 목록 추가
4. 위 과정을 처음부터 반복
## 요구사항
### 면접 자세
- 면접 진행 중 질문을 통해 요구사항 도출 및 모호한 부분 제거 필요
- 면접관 / 면접자 생각이 다를 수 있는 점에 유의
### 웹 크롤러 기본 속성 고려
- 규모 확장성: 병행성을 활용하여 수십억 개의 웹 페이지를 효과적으로 크롤링
- 안정성: 비정상적 입력 또는 환경(잘못 작성된 HTML / 무반응 서버 / 장애 / 악성 코드 포함 링크 등) 대응
- 예절: 짧은 시간 내 너무 많은 요청 전송 금지
- 확장성: 새로운 형태의 컨텐츠를 쉽게 추가 가능한 설계 필요
### 개략적 규모 추정
- 매달 10억 개의 웹 페이지 다운로드
- QPS = 10억 / 30일 / 24시간 / 3600초 = 약 400페이지/초
- 최대 QPS = 2 x QPS = 800
- 웹 페이지 평균 크기 500k로 가정
- 10억 페이지 x 500k = 500TB/월, 5년 보관 -> 30PB
# 2. 개략적 설계안 제시 및 동의 구하기
## 청사진
1. 시작 URL 집합
2. 미수집 URL 저장소
3. HTML 다운로더
4. 도메인 이름 변환기
5. 컨텐츠 파서
6. 중복 컨텐츠 확인
7. 컨텐츠 저장소
8. URL 추출기
9. URL 필터
10. URL 방문 여부 확인
11. URL 저장소
12. (2번 항목부터 다시 수행)
## 컴포넌트 기능
### 1. 시작 URL 집합
- 크롤링 시작 출발점
- 크롤러가 최대한 많은 링크를 탐색하게 하는 URL 선택 필요
- 전체 URL 공간을 작은 집합으로 분할
	- 지역적 특색
	- 주제별 (쇼핑 / 스포츠 / 건강 등)
### 2. 미수집 URL 저장소
- 다운로드 대상 URL을 저장 및 관리하는 FIFO 큐
### 3. HTML 다운로더
- 미수집 URL 저장소로부터 제공받은 URL 대상 웹 페이지 다운로드
### 4. 도메인 이름 변환기
- 웹 페이지 다운로드 대상 URL을 IP로 변환
### 5. 콘텐츠 파서
- 이상한 웹 페이지에 의해 저장 공간 낭비를 막고자 웹 페이지 파싱 / 검증
- 크롤링 서버 대상으로 구현 시, 크롤링 성능 저하가 있으므로 독릳뵌 컴포턴트로 제작
### 6. 중복 컨텐츠 확인
- 효율적인 자료 구조의 중복 컨텐츠 확인을 통한 데이터 중복 및 처리 시간 증가 최소화
### 7. 컨텐츠 저장소
- HTML 문서 보관 시스템
- 컨텐츠 유형에 따라 저장소 구현 방법 고려
	- 데이터 유형
	- 데이터 크기
	- 저장소 접근 빈도
	- 데이터 유효 기간
- 본 설계안은 디스크와 메모리 동시 사용
	- 데이터 양이 너무 많으므로, 대부분의 컨텐츠는 디스크에 저장
	- 인기 컨텐츠는 메모리로 관리하여 접근 지연시간 저감
### 8. URL 추출기
- HTML 파싱을 통한 링크 추출 수행
- 상대 경로를 절대 경로로 변환
### 9. URL 필터
- 크롤링 제외 대상 정보 관리
	- 특정 컨텐츠 타입
	- 파일 확장자 보유 URL
	- 접속 시 오류 발생 URL
	- 접근 제외 목록
### 10. URL 방문 여부 확인
- 기 방문 URL 확인 또는 미수집 URL 저장소에 보관된 URL 확인을 통한 중복 방문 방지
- 보통 블룸 필터 / 해시 테이블을 통해 구현
### 11. URL 저장소
- 방문한 URL 저장
## 웹 프롤러 작업 흐름
1. 시작 URL을 미수집 URL 저장소에 삽입
2. HTML 다운로더에서 미수집 URL 저장소 내 URL 목록 조회
3. HTML 다운로드 수행
	1. 도메인 이름 변환기를 통해 URL의 IP 주소 획득
	2. 웹 페이지 다운로드
4. 컨텐츠 파서가 HTML 페이지 파싱을 통해 페이지 유효성 검증
5. 컨텐츠 중복 여부 확인 개시
6. 컨텐츠 저장소 내 대상 페이지 존재 확인 (중복 컨텐츠 확인)
	- 존재 확인: 처리 미수행
	- 존재 미확인:
		1. 컨텐츠 저장
		2. URL 추출기 전달
7. URL 추출기가 HTML 페이지 내 링크 추출
8. 추출된 링크 URL 필터 전달
9. 필터링 완료 URL 대상 중복 URL 판별 단계 전달
10. URL 저장소 확인을 통해 URL 중복 처리 여부 확인 (중복 시 처리 미수행)
11. 미수집 URL 저장소에 대상 URL 전달
# 3. 상세 설계
## 대상 컴포넌트 / 구현 기술 검토
- DFS / BFS
- 미수집 URL 저장소
- HTML 다운로더
- 안정성 확보 전략
- 확장성 확보 전략
- 문제 있는 컨텐츠 감지 및 회피 전략
## DFS / BFS 고려
- 웹은 유향 그래프(directed graph) 형태 (페이지: 노드, 하이퍼링크: 에지)
- 웹의 경우, 그래프 크기가 커질 때 깊이 가늠이 어려우므로 BFS가 적절
### BFS 문제점
- 동일 서버 대상으로 다수의 링크 병렬 처리 시 예의 없는 크롤러가 됨
- 표준 BFS의 경우, 우선순위 없이 탐색 수행
	- 웹 사이트의 경우, 다양한 요소를 고려하여 우선순위 지정 필요
		- 페이지 순위
		- 사용자 트래픽 양
		- 업데이트 빈도
## 미수집 URL 저장소
- 미수집 URL 저장소를 통해 우선순위 및 신선도를 고려하여, 크롤링을 수행하는 예의를 갖춘 크롤러 구현 가능
### 예의
- 동일한 서버 대상으로 짧은 시간 안에 너무 많은 요청을 전달하는 것은 삼가야 함 (이는 무례한 일. DoS로 간주되기도 함)
- 동일 웹 사이트에 대해 한 번에 한 페이지만 요청해야 함
- 웹 사이트 호스트명<>작업 스레드(다운로드 수행) 사이의 관계 유지 필요
#### 구성
- 큐 라우터: 동일 호스트 내 URL은 언제나 같은 큐로 가도록 보장
- 매핑 테이블: 호스트 이름과 큐 사이의 관계 보관
- FIFO 큐: 동일 호스트에 속한 URL은 언제나 같은 큐에 보관
- 큐 선택기:
	1. 큐 순회
	2. 큐에서 URL 꺼내기
	3. 작업 스레드에 URL 전달
- 작업 스레드: 전달된 URL 다운로드 작업 수행 (작업 사이 일정 지연 시간을 둠)
#### 우선순위
- 유용성에 따라 URL 우선순위를 나누기 위해 다양한 척도 사용
	- 페이지랭크
	- 트래픽 양
	- 갱신 빈도
- 순위결정장치: URL 우선순위 결정 컴포넌트
##### 변경된 설계
- 우선순위결정장치: 입력된 URL에 대한 우선순위 결정
- 큐: 우선순위별로 큐 하나씩 할당 (우선순위가 높을 수록 선택 확률 증가)
- 큐 선택기:
	- 임의 큐에서 처리할 URL을 꺼내는 역할 담당
	- 순위가 높은 큐에서 더 자주 꺼내도록 프로그래밍
### 전체 설계
- 전면 큐: 우선순위 결정 과정 처리
- 후면 큐: 크롤러가 예의 바르게 동작하도록 보증
#### 신선도
- 웹 페이지의 추가 / 삭제 / 변경에 대해 신선함 유지를 위해 주기적으로 재수집 과정 필요
- 재수집 작업 최적화를 위한 전략 필요
	- 웹 페이지 변경 이력 활용
	- 우선순위 활용을 통해, 중요한 페이지는 더 자주 재수집
#### 미수집 URL 저장소를 위한 지속적 저장 장치
- 크롤러가 수집한 방대한 데이터를 저장하기 위한 전략 필요
	- 메모리에 모두 저장: 안정성 / 규모 확장성이 나쁨
	- 디스크에 모두 저장: 성능이 나쁨
- 본 설계안은 절충안 채택
	- 대부분의 URL: 디스크에 기록
	- 메모리 버퍼: IO 비용 저감을 위한 큐 관리 (주기적으로 디스크에 flush)
## HTML 다운로더
- HTTP 프로토콜을 통해 웹 페이지 다운로드
### Robots.txt
- 로봇 제외 프로토콜
- 웹 사이트에서 크롤러 수집 규칙 정의
	- 예시
		- `User-agent: Googlebot`
		- `Disallow: /creatorhub/*`
		- `Disallow: /rss/people/*/reviews`
### 성능 최적화
#### 1. 분산 크롤링
- 크롤링 작업을 여러 서버에 분산
- URL 공간을 작은 단위로 분할하여, 각 서버는 그 중 일부의 다운로드를 담당
#### 2. 도메인 이름 변환 결과 캐시
- 도메인 이름 변환은 DNS의 요청 -> 결과 순서의 동기적 특성으로 인해 동시 처리 불가
- 도메인 이름<>IP주소 관계를 캐시에 보관 / cron job을 통해 갱신하여 성능 보완 필요
#### 3. 지역성
- 크롤러 각 요소를를 크롤링 대상 서버와 지역적으로 가깝게 배치하여 다운로드 속도 향상
	- 크롤 서버
	- 캐시
	- 큐
	- 저장소
#### 4. 짧은 타임아웃
- 웹 서버의 응답 지연 / 응답 불가 대응을 위한 대기시간 설정
### 안정성
- 안정 해시: 다운로더 서버들에 부하 분산을 위해 적용 가능 (서버 추가 / 삭제 대응)
- 크롤링 상태 및 수집 데이터 저장: 장애 발생 시 원활한 복구를 위해 지속적으로 크롤링 상태 및 수집된 데이터를 저장장치에 기록
- 예외 처리: 대규모 시스템에서 불가피한 에러로 인해 전체 시스템이 중단되지 않도록 우아한 예외 처리 필요
- 데이터 검증: 시스템 오류 방지를 위한 수단
### 확장성
- 새로운 컨텐츠를 쉽게 지원하기 위해 확장 모듈 형태로 컨텐츠 지원
	- 예시:
		- PNG 다운로더: PNG 파일 다운로드 플러그인
		- 웹 모니터: 저작권 / 상표권 침해 방지를 위한 웹 모니터링 지원 모듈
### 문제 있는 컨텐츠 감지 및 회피
#### 1. 중복 컨텐츠
- 해시 / 체크섬을 통해 웹 컨텐츠 30% 가량의 중복 컨텐츠 탐지
#### 2. 거미 덫
- 정의: 크롤러가 무한 루프에 빠지도록 설계한 웹 페이지
	- 예시: 무한 depth의 디렉토리 구조 링크
- 자동으로 덫을 피하는 알고리즘을 만드는 것은 까다로우므로, 사람이 수작업으로 대응하는 방법 존재
	- 크롤러 탐색 대상에서 제외
	- URL 필터 목록에 추가
#### 3. 데이터 노이즈
- 가치 없는 광고 / 스크립트 코드 / 스팸 URL 등을 크롤러 탐색 대상에서 제외
# 4. 마무리
## 면접관과 추가 논의
- 서버 측 렌더링: JS / AJAX 기술을 통한 URL 동적 생성 링크 탐지를 위해 서버 측 렌더링 적용
- 원치 않는 페이지 필터링: 스팸 방지 컴포넌트를 통해 품질이 조악하거나, 스팸성인 페이지를 걸러내어 크롤링 자원 감소
- 데이터베이스 다중화 및 샤딩: 다중화 / 샤딩을 통해 데이터 계층의 가용성, 규모 확장성, 안정성 향상
- 수평적 규모 확장성: 무상태 서버로 만들어 규모 대규모 크롤링 실현
- 가용성, 일관성, 안정성: 성공적인 대형 시스템을 만들기 위해 필수적으로 고려
- 데이터 분석 솔루션: 데이터 수집 / 분석을 통해 시스템을 세밀하게 조정